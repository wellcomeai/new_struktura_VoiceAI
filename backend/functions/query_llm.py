# backend/functions/query_llm.py
"""
–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ ChatGPT API —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞.
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –∫–∞–∫ –∫–ª–∞—Å—Å FunctionBase –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≤ —Å–∏—Å—Ç–µ–º–µ.

üÜï v3.0: –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ OpenAI Assistants API —Å Threads –∏ WebSocket streaming
"""

import asyncio
from typing import Dict, Any

from backend.core.logging import get_logger
from backend.functions.base import FunctionBase
from backend.functions.registry import register_function
from backend.services.openai_assistant import (
    create_thread,
    add_message_to_thread,
    stream_assistant_response,
    get_or_create_assistant
)

logger = get_logger(__name__)


@register_function
class QueryLLMFunction(FunctionBase):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–π LLM –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ OpenAI Assistants API"""

    @classmethod
    def get_name(cls) -> str:
        return "query_llm"

    @classmethod
    def get_description(cls) -> str:
        return "–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–π LLM –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤"

    @classmethod
    def get_parameters(cls) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "prompt": {
                    "type": "string",
                    "description": "–¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –∏–ª–∏ –∑–∞–¥–∞—á–∏ –¥–ª—è LLM –º–æ–¥–µ–ª–∏"
                }
            },
            "required": ["prompt"]
        }

    @staticmethod
    async def execute(arguments: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Streaming LLM —á–µ—Ä–µ–∑ OpenAI Assistants API —Å Thread management.

        –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê (v3.0):
        - –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAI Assistants API –≤–º–µ—Å—Ç–æ Chat Completions
        - Thread —Ö—Ä–∞–Ω–∏—Ç—Å—è –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ OpenAI (–Ω–µ –≤ localStorage)
        - thread_id –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ WebSocket –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ sessionStorage
        - Streaming –∏–¥–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ WebSocket (–Ω–µ —á–µ—Ä–µ–∑ HTTP)
        - –ò—Å—Ç–æ—Ä–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è OpenAI Threads

        Args:
            arguments: Function arguments with 'prompt'
            context: Execution context with websocket and thread_id

        Returns:
            Success status with thread_id
        """
        try:
            prompt = arguments.get("prompt")

            if not prompt:
                error_msg = "Prompt is required"
                logger.error(f"[QUERY_LLM] {error_msg}")
                return {"error": error_msg, "status": "error"}

            logger.info(f"[QUERY_LLM] üöÄ Processing LLM query: {prompt[:100]}...")

            # –ü–æ–ª—É—á–∞–µ–º WebSocket –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            websocket = context.get("websocket") if context else None

            if not websocket:
                logger.error("[QUERY_LLM] ‚ùå No WebSocket in context")
                return {
                    "error": "WebSocket not available",
                    "status": "error"
                }

            # –ü–æ–ª—É—á–∞–µ–º thread_id –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            thread_id = context.get("thread_id") if context else None

            # üÜï –ü–æ–ª—É—á–∞–µ–º Assistant ID
            assistant_id = await get_or_create_assistant()

            # üÜï –°–æ–∑–¥–∞–µ–º Thread –µ—Å–ª–∏ –Ω–µ—Ç
            if not thread_id:
                logger.info("[QUERY_LLM] üÜï Creating new thread...")
                thread_id = await create_thread()

                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º thread_id –Ω–∞ frontend –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ sessionStorage
                await websocket.send_json({
                    "type": "thread_created",
                    "thread_id": thread_id
                })

                # –û–±–Ω–æ–≤–ª—è–µ–º context –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –≤—ã–∑–æ–≤–æ–≤
                if context:
                    context["thread_id"] = thread_id

                logger.info(f"[QUERY_LLM] ‚úÖ New thread created: {thread_id}")
            else:
                logger.info(f"[QUERY_LLM] ‚ôªÔ∏è  Using existing thread: {thread_id}")

            # üÜï –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ Thread
            logger.info(f"[QUERY_LLM] üí¨ Adding message to thread {thread_id}: {prompt[:50]}...")
            await add_message_to_thread(thread_id, prompt)

            # üÜï Streaming –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ WebSocket
            logger.info("[QUERY_LLM] üåä Starting streaming response...")
            full_response = ""
            chunk_count = 0

            async for chunk in stream_assistant_response(thread_id, assistant_id):
                full_response += chunk
                chunk_count += 1

                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º chunk —á–µ—Ä–µ–∑ WebSocket
                await websocket.send_json({
                    "type": "llm_response_chunk",
                    "chunk": chunk
                })

            # üÜï –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ streaming
            await websocket.send_json({
                "type": "llm_response_complete",
                "thread_id": thread_id
            })

            logger.success(
                f"[QUERY_LLM] ‚úÖ Streaming complete. "
                f"Chunks: {chunk_count}, Length: {len(full_response)} chars"
            )

            return {
                "status": "success",
                "thread_id": thread_id,
                "response_length": len(full_response),
                "chunks_sent": chunk_count,
                "result": f"Streaming response sent ({len(full_response)} chars)"
            }

        except Exception as e:
            error_msg = f"Error in LLM query: {str(e)}"
            logger.error(f"[QUERY_LLM] ‚ùå {error_msg}")

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –Ω–∞ frontend
            if context and context.get("websocket"):
                try:
                    await context["websocket"].send_json({
                        "type": "llm_error",
                        "error": str(e)
                    })
                except Exception as ws_error:
                    logger.error(f"[QUERY_LLM] Failed to send error to WebSocket: {ws_error}")

            return {
                "error": error_msg,
                "status": "error"
            }
