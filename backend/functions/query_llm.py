# backend/functions/query_llm.py
"""
–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ ChatGPT API —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞.
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –∫–∞–∫ –∫–ª–∞—Å—Å FunctionBase –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≤ —Å–∏—Å—Ç–µ–º–µ.
"""

import openai
import asyncio
import time  # üÜï –ù–û–í–û–ï
from typing import Dict, Any

from backend.core.config import settings
from backend.core.logging import get_logger
from backend.functions.base import FunctionBase
from backend.functions.registry import register_function

logger = get_logger(__name__)


@register_function
class QueryLLMFunction(FunctionBase):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–π LLM –º–æ–¥–µ–ª–∏"""
    
    @classmethod
    def get_name(cls) -> str:
        return "query_llm"
    
    @classmethod
    def get_description(cls) -> str:
        return "–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–π LLM –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤"
    
    @classmethod
    def get_parameters(cls) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "prompt": {
                    "type": "string",
                    "description": "–¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –∏–ª–∏ –∑–∞–¥–∞—á–∏ –¥–ª—è LLM –º–æ–¥–µ–ª–∏"
                },
                "model": {
                    "type": "string", 
                    "description": "–ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (gpt-4, gpt-3.5-turbo)",
                    "default": "gpt-4o-mini"
                }
            },
            "required": ["prompt"]
        }
    
    @staticmethod
    async def execute(arguments: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Prepare LLM query for client-side HTTP streaming.

        –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê:
        - –ù–µ –≤—ã–∑—ã–≤–∞–µ–º OpenAI –Ω–∞–ø—Ä—è–º—É—é (—ç–∫–æ–Ω–æ–º–∏–º –≤—Ä–µ–º—è)
        - –û—Ç–ø—Ä–∞–≤–ª—è–µ–º trigger —Å–æ–±—ã—Ç–∏–µ –∫–ª–∏–µ–Ω—Ç—É —á–µ—Ä–µ–∑ WebSocket
        - –ö–ª–∏–µ–Ω—Ç –¥–µ–ª–∞–µ—Ç HTTP –∑–∞–ø—Ä–æ—Å –∫ /api/llm/stream —Å –∏—Å—Ç–æ—Ä–∏–µ–π –∏–∑ localStorage
        - –ò—Å—Ç–æ—Ä–∏—è —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –∫–ª–∏–µ–Ω—Ç–æ–º, —Å–µ—Ä–≤–µ—Ä –µ—ë –ù–ï —Ö—Ä–∞–Ω–∏—Ç

        Args:
            arguments: Function arguments with 'prompt'
            context: Execution context with websocket and session_id

        Returns:
            Success status (actual LLM response goes via HTTP streaming)
        """
        try:
            prompt = arguments.get("prompt")

            if not prompt:
                error_msg = "Prompt is required"
                logger.error(f"[QUERY_LLM] {error_msg}")
                return {"error": error_msg, "status": "error"}

            logger.info(f"[QUERY_LLM] üöÄ Preparing LLM query: {prompt[:100]}...")

            # –ü–æ–ª—É—á–∞–µ–º session_id –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            session_id = context.get("session_id") if context else None

            if not session_id:
                logger.warning("[QUERY_LLM] ‚ö†Ô∏è No session_id in context")
            else:
                logger.info(f"[QUERY_LLM] Session ID: {session_id}")

            # –ü–æ–ª—É—á–∞–µ–º WebSocket –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            websocket = context.get("websocket") if context else None

            if websocket:
                # üÜï –û—Ç–ø—Ä–∞–≤–ª—è–µ–º trigger —Å–æ–±—ã—Ç–∏–µ –∫–ª–∏–µ–Ω—Ç—É
                await websocket.send_json({
                    "type": "llm_query.trigger",
                    "query": prompt,
                    "session_id": session_id,
                    "timestamp": time.time()
                })

                logger.info(f"[QUERY_LLM] ‚úÖ Trigger event sent to client")
                logger.info(f"[QUERY_LLM] üì± Client will handle HTTP streaming with localStorage history")
            else:
                logger.error("[QUERY_LLM] ‚ùå No WebSocket in context")
                return {
                    "error": "WebSocket not available",
                    "status": "error"
                }

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É—Å–ø–µ—Ö
            # –†–µ–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç LLM –ø—Ä–∏–¥–µ—Ç —á–µ—Ä–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã–π HTTP streaming endpoint
            return {
                "status": "triggered",
                "result": "LLM query triggered. Client will stream response via HTTP.",
                "session_id": session_id,
                "query": prompt[:50] + "..." if len(prompt) > 50 else prompt
            }

        except Exception as e:
            error_msg = f"Error preparing LLM query: {str(e)}"
            logger.error(f"[QUERY_LLM] ‚ùå {error_msg}")
            return {
                "error": error_msg,
                "status": "error"
            }
