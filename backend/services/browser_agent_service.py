"""
üöÄ LLM Streaming Service v2.1 (OPTIMIZED)
==========================================

–°–µ—Ä–≤–∏—Å –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç OpenAI Chat API.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—ã–≤–æ–¥–∞ —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —ç–∫—Ä–∞–Ω
–ø—Ä–∏ –≥–æ–ª–æ—Å–æ–≤–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —á–µ—Ä–µ–∑ Gemini.

üîß v2.1 OPTIMIZATION:
‚úÖ Buffered delta sending (batches instead of per-token)
‚úÖ Reduced WebSocket message frequency
‚úÖ Prevents audio distortion during streaming

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
- Gemini Live API ‚Äî –≥–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥/–≤—ã–≤–æ–¥
- OpenAI Chat API (stream=True) ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
- WebSocket ‚Äî –¥–æ—Å—Ç–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏

–°–û–ë–´–¢–ò–Ø WebSocket:
- llm.stream.start   ‚Äî –Ω–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- llm.stream.delta   ‚Äî chunk —Ç–µ–∫—Å—Ç–∞ (BUFFERED)
- llm.stream.done    ‚Äî –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- llm.stream.error   ‚Äî –æ—à–∏–±–∫–∞

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
    from backend.services.browser_agent_service import get_browser_agent_service
    
    llm_service = get_browser_agent_service()
    result = await llm_service.stream_response(
        query="–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
        websocket=websocket
    )
    # result = {"success": True, "phrase": "–ì–æ—Ç–æ–≤–æ, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–∞ —ç–∫—Ä–∞–Ω–µ", ...}
"""

import asyncio
import json
import uuid
import time
import random
import os
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
import aiohttp

from backend.core.logging import get_logger
from backend.core.config import settings

logger = get_logger(__name__)


# ============================================================================
# CONFIGURATION
# ============================================================================

class LLMConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LLM Streaming Service"""
    
    # –ú–æ–¥–µ–ª—å OpenAI –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
    MODEL = "gpt-4o-mini"
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
    MAX_TOKENS = 4096
    
    # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å)
    TEMPERATURE = 0.1
    
    # –¢–∞–π–º–∞—É—Ç—ã
    REQUEST_TIMEOUT = 60.0  # –û–±—â–∏–π —Ç–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞
    CONNECT_TIMEOUT = 10.0  # –¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    
    # OpenAI API
    OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"
    
    # üÜï v2.1: Buffering settings
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π (—Å–∏–º–≤–æ–ª—ã)
    BUFFER_MIN_CHARS = 50
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –±—É—Ñ–µ—Ä–∞ (—Å–µ–∫—É–Ω–¥—ã)
    BUFFER_MAX_WAIT = 0.3
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏/–∞–±–∑–∞—Ü–∞
    FLUSH_ON_SENTENCE = True


# ============================================================================
# SYSTEM PROMPT
# ============================================================================

SYSTEM_PROMPT = """–¢—ã ‚Äî —É–º–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É.

–ü—Ä–∞–≤–∏–ª–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:
- –ò—Å–ø–æ–ª—å–∑—É–π markdown –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞
- –ó–∞–≥–æ–ª–æ–≤–∫–∏: ## –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤, ### –¥–ª—è –ø–æ–¥—Ä–∞–∑–¥–µ–ª–æ–≤
- –°–ø–∏—Å–∫–∏: - –∏–ª–∏ 1. 2. 3. –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π
- –ö–æ–¥: ```—è–∑—ã–∫ –¥–ª—è –±–ª–æ–∫–æ–≤ –∫–æ–¥–∞, `–∫–æ–¥` –¥–ª—è inline
- –í—ã–¥–µ–ª—è–π **–≤–∞–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã** –∂–∏—Ä–Ω—ã–º
- –†–∞–∑–¥–µ–ª—è–π –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏ –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏

–ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–≤–µ—Ç–æ–≤:
- –û—Ç–≤–µ—á–∞–π –Ω–∞ —è–∑—ã–∫–µ –≤–æ–ø—Ä–æ—Å–∞ (—Ä—É—Å—Å–∫–∏–π/–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º
- –ü—Ä–∏–≤–æ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã –≥–¥–µ —É–º–µ—Å—Ç–Ω–æ
- –ò–∑–±–µ–≥–∞–π –≤–æ–¥—ã –∏ –æ–±—â–∏—Ö —Ñ—Ä–∞–∑
- –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ—è—Å–µ–Ω ‚Äî —É—Ç–æ—á–Ω–∏

–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –∫–æ—Ç–æ—Ä—ã–π –æ–±—â–∞–µ—Ç—Å—è –≥–æ–ª–æ—Å–æ–º. –¢–≤–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω –Ω–∞ —ç–∫—Ä–∞–Ω–µ."""


# ============================================================================
# RESPONSE PHRASES (–¥–ª—è Gemini –æ–∑–≤—É—á–∫–∏)
# ============================================================================

RESPONSE_PHRASES = [
    "–ì–æ—Ç–æ–≤–æ, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–∞ —ç–∫—Ä–∞–Ω–µ",
    "–û—Ç–≤–µ—Ç –≤—ã–≤–µ–¥–µ–Ω –Ω–∞ —ç–∫—Ä–∞–Ω",
    "–°–º–æ—Ç—Ä–∏ –Ω–∞ —ç–∫—Ä–∞–Ω, —Ç–∞–º –≤—Å—ë –ø–æ–¥—Ä–æ–±–Ω–æ",
    "–í—ã–≤–µ–ª —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç",
    "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–µ—Ä–µ–¥ —Ç–æ–±–æ–π",
    "–ì–æ—Ç–æ–≤–æ, –º–æ–∂–µ—à—å –∏–∑—É—á–∏—Ç—å –Ω–∞ —ç–∫—Ä–∞–Ω–µ",
    "–ü–æ–∫–∞–∑–∞–ª –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç",
    "–û—Ç–≤–µ—Ç –Ω–∞ —ç–∫—Ä–∞–Ω–µ",
    "–í–æ—Ç —á—Ç–æ —è –Ω–∞—à—ë–ª, —Å–º–æ—Ç—Ä–∏ –Ω–∞ —ç–∫—Ä–∞–Ω",
    "–ü–æ–¥–≥–æ—Ç–æ–≤–∏–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ–Ω–∞ –Ω–∞ —ç–∫—Ä–∞–Ω–µ",
]

ERROR_PHRASES = [
    "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞, –ø–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑",
    "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç, –ø–æ–≤—Ç–æ—Ä–∏ –≤–æ–ø—Ä–æ—Å",
    "–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –ø–æ–ø—Ä–æ–±—É–π —Å–Ω–æ–≤–∞",
    "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ, –ø–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å",
]


# ============================================================================
# DATA CLASSES
# ============================================================================

@dataclass
class StreamResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç—Ä–∏–º–∏–Ω–≥–∞"""
    success: bool
    phrase: str  # –§—Ä–∞–∑–∞ –¥–ª—è –æ–∑–≤—É—á–∫–∏ Gemini
    full_content: str = ""
    error: Optional[str] = None
    tokens_used: int = 0
    duration_ms: int = 0
    model: str = LLMConfig.MODEL


# ============================================================================
# LLM STREAMING SERVICE
# ============================================================================

class BrowserAgentService:
    """
    LLM Streaming Service v2.1 (OPTIMIZED)
    
    –°—Ç—Ä–∏–º–∏—Ç –æ—Ç–≤–µ—Ç—ã –æ—Ç OpenAI Chat API –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ —á–µ—Ä–µ–∑ WebSocket.
    üÜï v2.1: –ë—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è –¥–µ–ª—å—Ç –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∏—Å–∫–∞–∂–µ–Ω–∏—è –∞—É–¥–∏–æ.
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        self.api_key = os.environ.get('OPENAI_API_KEY')
        
        if not self.api_key:
            logger.warning("[LLM-STREAM] ‚ö†Ô∏è OPENAI_API_KEY not found in environment")
        
        logger.info(f"[LLM-STREAM] ‚úÖ Service initialized (v2.1 OPTIMIZED)")
        logger.info(f"[LLM-STREAM]    Model: {LLMConfig.MODEL}")
        logger.info(f"[LLM-STREAM]    Max tokens: {LLMConfig.MAX_TOKENS}")
        logger.info(f"[LLM-STREAM]    Buffer min chars: {LLMConfig.BUFFER_MIN_CHARS}")
        logger.info(f"[LLM-STREAM]    Buffer max wait: {LLMConfig.BUFFER_MAX_WAIT}s")
    
    # ========================================================================
    # MAIN METHOD: Stream Response (OPTIMIZED v2.1)
    # ========================================================================
    
    async def stream_response(
        self,
        query: str,
        websocket: Any,
        model: str = None,
        system_prompt: str = None,
        max_tokens: int = None,
        temperature: float = None
    ) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ ‚Äî —Å—Ç—Ä–∏–º–∏—Ç –æ—Ç–≤–µ—Ç –æ—Ç OpenAI –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥.
        
        üÜï v2.1: –ë—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ WebSocket.
        
        Args:
            query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            websocket: WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–±—ã—Ç–∏–π
            model: –ú–æ–¥–µ–ª—å OpenAI (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é gpt-4o-mini)
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
        """
        request_id = f"req_{uuid.uuid4().hex[:12]}"
        start_time = time.time()
        full_content = ""
        tokens_used = 0
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        _model = model or LLMConfig.MODEL
        _system_prompt = system_prompt or SYSTEM_PROMPT
        _max_tokens = max_tokens or LLMConfig.MAX_TOKENS
        _temperature = temperature or LLMConfig.TEMPERATURE
        
        logger.info(f"[LLM-STREAM] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
        logger.info(f"[LLM-STREAM] üöÄ STREAM START (v2.1 BUFFERED)")
        logger.info(f"[LLM-STREAM]    Request ID: {request_id}")
        logger.info(f"[LLM-STREAM]    Query: {query[:100]}{'...' if len(query) > 100 else ''}")
        logger.info(f"[LLM-STREAM]    Model: {_model}")
        logger.info(f"[LLM-STREAM] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
        
        # üÜï Buffer state
        buffer = ""
        last_flush_time = time.time()
        messages_sent = 0
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á
            if not self.api_key:
                raise ValueError("OpenAI API key not configured")
            
            # 1. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ –Ω–∞—á–∞–ª–∞
            await self._send_event(websocket, {
                "type": "llm.stream.start",
                "request_id": request_id,
                "query": query,
                "model": _model
            })
            
            # 2. –°—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç –æ—Ç OpenAI —Å –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–µ–π
            async for chunk_data in self._call_openai_stream(
                query=query,
                model=_model,
                system_prompt=_system_prompt,
                max_tokens=_max_tokens,
                temperature=_temperature
            ):
                if chunk_data.get("type") == "content":
                    content = chunk_data.get("content", "")
                    full_content += content
                    buffer += content
                    
                    # üÜï v2.1: –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –±—É—Ñ–µ—Ä–∞
                    current_time = time.time()
                    time_since_flush = current_time - last_flush_time
                    
                    should_flush = False
                    
                    # –£—Å–ª–æ–≤–∏–µ 1: –ë—É—Ñ–µ—Ä –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π
                    if len(buffer) >= LLMConfig.BUFFER_MIN_CHARS:
                        should_flush = True
                    
                    # –£—Å–ª–æ–≤–∏–µ 2: –ü—Ä–æ—à–ª–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏
                    if time_since_flush >= LLMConfig.BUFFER_MAX_WAIT and len(buffer) > 0:
                        should_flush = True
                    
                    # –£—Å–ª–æ–≤–∏–µ 3: –ö–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Ç–æ—á–∫–∞, ?, !, –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏)
                    if LLMConfig.FLUSH_ON_SENTENCE and len(buffer) > 10:
                        if buffer.rstrip().endswith(('.', '!', '?', '\n', '„ÄÇ', 'ÔºÅ', 'Ôºü')):
                            should_flush = True
                    
                    if should_flush:
                        await self._send_event(websocket, {
                            "type": "llm.stream.delta",
                            "request_id": request_id,
                            "content": buffer
                        })
                        messages_sent += 1
                        buffer = ""
                        last_flush_time = current_time
                        
                        # üÜï –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä
                        await asyncio.sleep(0.01)
                
                elif chunk_data.get("type") == "usage":
                    tokens_used = chunk_data.get("total_tokens", 0)
            
            # 3. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫ –±—É—Ñ–µ—Ä–∞
            if buffer:
                await self._send_event(websocket, {
                    "type": "llm.stream.delta",
                    "request_id": request_id,
                    "content": buffer
                })
                messages_sent += 1
            
            # 4. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            duration_ms = int((time.time() - start_time) * 1000)
            
            await self._send_event(websocket, {
                "type": "llm.stream.done",
                "request_id": request_id,
                "full_content": full_content,
                "model": _model,
                "usage": {
                    "total_tokens": tokens_used
                },
                "duration_ms": duration_ms
            })
            
            # –í—ã–±–∏—Ä–∞–µ–º —Ä–∞–Ω–¥–æ–º–Ω—É—é —Ñ—Ä–∞–∑—É –¥–ª—è Gemini
            phrase = self.get_random_phrase()
            
            logger.info(f"[LLM-STREAM] ‚úÖ STREAM COMPLETE (v2.1)")
            logger.info(f"[LLM-STREAM]    Duration: {duration_ms}ms")
            logger.info(f"[LLM-STREAM]    Content length: {len(full_content)} chars")
            logger.info(f"[LLM-STREAM]    Tokens: {tokens_used}")
            logger.info(f"[LLM-STREAM]    Messages sent: {messages_sent} (buffered)")
            logger.info(f"[LLM-STREAM]    Phrase: {phrase}")
            
            return {
                "success": True,
                "phrase": phrase,
                "full_content": full_content,
                "tokens_used": tokens_used,
                "duration_ms": duration_ms,
                "model": _model,
                "request_id": request_id,
                "messages_sent": messages_sent
            }
            
        except asyncio.TimeoutError:
            error_msg = "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞"
            logger.error(f"[LLM-STREAM] ‚ùå TIMEOUT: {error_msg}")
            
            await self._send_error(websocket, request_id, "timeout", error_msg)
            
            return {
                "success": False,
                "phrase": self.get_error_phrase(),
                "error": error_msg,
                "full_content": full_content,
                "duration_ms": int((time.time() - start_time) * 1000),
                "model": _model,
                "request_id": request_id
            }
            
        except aiohttp.ClientError as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ OpenAI: {str(e)}"
            logger.error(f"[LLM-STREAM] ‚ùå CONNECTION ERROR: {error_msg}")
            
            await self._send_error(websocket, request_id, "connection_error", error_msg)
            
            return {
                "success": False,
                "phrase": self.get_error_phrase(),
                "error": error_msg,
                "full_content": full_content,
                "duration_ms": int((time.time() - start_time) * 1000),
                "model": _model,
                "request_id": request_id
            }
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"[LLM-STREAM] ‚ùå ERROR: {error_msg}")
            
            await self._send_error(websocket, request_id, "internal_error", error_msg)
            
            return {
                "success": False,
                "phrase": self.get_error_phrase(),
                "error": error_msg,
                "full_content": full_content,
                "duration_ms": int((time.time() - start_time) * 1000),
                "model": _model,
                "request_id": request_id
            }
    
    # ========================================================================
    # OpenAI Streaming Call
    # ========================================================================
    
    async def _call_openai_stream(
        self,
        query: str,
        model: str,
        system_prompt: str,
        max_tokens: int,
        temperature: float
    ):
        """
        –í—ã–∑–æ–≤ OpenAI Chat Completions API —Å–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º.
        
        Yields:
            Dict —Å —Ç–∏–ø–æ–º —Å–æ–±—ã—Ç–∏—è:
            - {"type": "content", "content": "—Ç–µ–∫—Å—Ç"}
            - {"type": "usage", "total_tokens": 123}
        """
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": True,
            "stream_options": {"include_usage": True}  # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è usage –≤ –∫–æ–Ω—Ü–µ
        }
        
        timeout = aiohttp.ClientTimeout(
            total=LLMConfig.REQUEST_TIMEOUT,
            connect=LLMConfig.CONNECT_TIMEOUT
        )
        
        logger.info(f"[LLM-STREAM] üì§ Calling OpenAI API...")
        logger.info(f"[LLM-STREAM]    URL: {LLMConfig.OPENAI_API_URL}")
        logger.info(f"[LLM-STREAM]    Model: {model}")
        
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.post(
                LLMConfig.OPENAI_API_URL,
                headers=headers,
                json=payload
            ) as response:
                
                if response.status != 200:
                    error_text = await response.text()
                    logger.error(f"[LLM-STREAM] ‚ùå OpenAI API error: {response.status}")
                    logger.error(f"[LLM-STREAM]    Response: {error_text[:500]}")
                    
                    # –ü–∞—Ä—Å–∏–º –æ—à–∏–±–∫—É OpenAI
                    try:
                        error_data = json.loads(error_text)
                        error_message = error_data.get("error", {}).get("message", error_text)
                    except:
                        error_message = error_text
                    
                    raise Exception(f"OpenAI API error ({response.status}): {error_message}")
                
                logger.info(f"[LLM-STREAM] üì• Streaming response...")
                
                # –ß–∏—Ç–∞–µ–º SSE –ø–æ—Ç–æ–∫
                async for line in response.content:
                    line = line.decode('utf-8').strip()
                    
                    if not line:
                        continue
                    
                    if line.startswith("data: "):
                        data_str = line[6:]  # –£–±–∏—Ä–∞–µ–º "data: "
                        
                        if data_str == "[DONE]":
                            logger.info(f"[LLM-STREAM] üì• Stream finished")
                            break
                        
                        try:
                            data = json.loads(data_str)
                            
                            # –ü–æ–ª—É—á–∞–µ–º content –∏–∑ delta
                            choices = data.get("choices", [])
                            if choices:
                                delta = choices[0].get("delta", {})
                                content = delta.get("content")
                                
                                if content:
                                    yield {"type": "content", "content": content}
                            
                            # –ü–æ–ª—É—á–∞–µ–º usage (–ø—Ä–∏—Ö–æ–¥–∏—Ç –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º chunk)
                            usage = data.get("usage")
                            if usage:
                                yield {
                                    "type": "usage",
                                    "prompt_tokens": usage.get("prompt_tokens", 0),
                                    "completion_tokens": usage.get("completion_tokens", 0),
                                    "total_tokens": usage.get("total_tokens", 0)
                                }
                                
                        except json.JSONDecodeError as e:
                            logger.warning(f"[LLM-STREAM] ‚ö†Ô∏è Failed to parse chunk: {data_str[:100]}")
                            continue
    
    # ========================================================================
    # Helper Methods
    # ========================================================================
    
    async def _send_event(self, websocket: Any, event: Dict) -> bool:
        """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–±—ã—Ç–∏–µ —á–µ—Ä–µ–∑ WebSocket"""
        try:
            await websocket.send_json(event)
            return True
        except Exception as e:
            logger.error(f"[LLM-STREAM] ‚ùå Failed to send event: {e}")
            return False
    
    async def _send_error(
        self,
        websocket: Any,
        request_id: str,
        error_code: str,
        message: str
    ) -> bool:
        """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–±—ã—Ç–∏–µ –æ—à–∏–±–∫–∏"""
        return await self._send_event(websocket, {
            "type": "llm.stream.error",
            "request_id": request_id,
            "error_code": error_code,
            "message": message
        })
    
    def get_random_phrase(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–Ω–¥–æ–º–Ω—É—é —Ñ—Ä–∞–∑—É –¥–ª—è –æ–∑–≤—É—á–∫–∏ Gemini"""
        return random.choice(RESPONSE_PHRASES)
    
    def get_error_phrase(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–Ω–¥–æ–º–Ω—É—é —Ñ—Ä–∞–∑—É –æ—à–∏–±–∫–∏ –¥–ª—è –æ–∑–≤—É—á–∫–∏ Gemini"""
        return random.choice(ERROR_PHRASES)


# ============================================================================
# FACTORY FUNCTIONS (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
# ============================================================================

# Singleton instance
_service_instance: Optional[BrowserAgentService] = None


def get_browser_agent_service(api_key: str = None) -> BrowserAgentService:
    """
    –ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä LLM Streaming Service.
    
    Args:
        api_key: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    
    Returns:
        BrowserAgentService instance
    """
    global _service_instance
    
    if _service_instance is None:
        _service_instance = BrowserAgentService()
    
    return _service_instance


def create_browser_agent_service() -> BrowserAgentService:
    """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞"""
    return BrowserAgentService()


# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
browser_agent_service: Optional[BrowserAgentService] = None
