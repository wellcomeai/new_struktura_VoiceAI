"""
üöÄ LLM Stream WebSocket Handler v3.0
=====================================

–û—Ç–¥–µ–ª—å–Ω—ã–π WebSocket —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è LLM —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞.
–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω –æ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∏—Å–∫–∞–∂–µ–Ω–∏–π –∞—É–¥–∏–æ.

üîß v2.0: OpenAI API key from User model via assistant_id chain:
    assistant_id ‚Üí GeminiAssistantConfig ‚Üí user_id ‚Üí User ‚Üí openai_api_key

üîß v3.0: Chat history support (5 pairs = 10 messages context)

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Browser   ‚îÇ   WS    ‚îÇ   LLM Stream     ‚îÇ
‚îÇ             ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   Handler        ‚îÇ
‚îÇ  (text UI)  ‚îÇ         ‚îÇ  (OpenAI API)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–°–û–ë–´–¢–ò–Ø:
Client ‚Üí Server:
- llm.query: –ó–∞–ø—Ä–æ—Å –∫ LLM (—Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π)
  {
    "type": "llm.query",
    "query": "—Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å",
    "history": [
      {"role": "user", "content": "..."},
      {"role": "assistant", "content": "..."}
    ],
    "request_id": "text_123"
  }

Server ‚Üí Client:
- llm.stream.start: –ù–∞—á–∞–ª–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
- llm.stream.delta: Chunk —Ç–µ–∫—Å—Ç–∞
- llm.stream.done: –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
- llm.stream.error: –û—à–∏–±–∫–∞
"""

from fastapi import WebSocket, WebSocketDisconnect
from sqlalchemy.orm import Session
import json
import asyncio
import uuid
import time
import os
import aiohttp
from typing import Optional, Dict, Any, List

from backend.core.logging import get_logger
from backend.models.gemini_assistant import GeminiAssistantConfig
from backend.models.user import User

logger = get_logger(__name__)


# ============================================================================
# CONFIGURATION
# ============================================================================

class LLMStreamConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LLM Stream Handler"""
    MODEL = "gpt-4o-mini"
    MAX_TOKENS = 4096
    TEMPERATURE = 0.1
    REQUEST_TIMEOUT = 60.0
    CONNECT_TIMEOUT = 10.0
    OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"
    
    # –ë—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
    BUFFER_MIN_CHARS = 30
    BUFFER_MAX_WAIT = 0.2
    
    # üÜï v3.0: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
    MAX_HISTORY_MESSAGES = 10  # 5 –ø–∞—Ä


SYSTEM_PROMPT = """–¢—ã ‚Äî —É–º–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É.

–ü—Ä–∞–≤–∏–ª–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:
- –ò—Å–ø–æ–ª—å–∑—É–π markdown –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞
- –ó–∞–≥–æ–ª–æ–≤–∫–∏: ## –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤, ### –¥–ª—è –ø–æ–¥—Ä–∞–∑–¥–µ–ª–æ–≤
- –°–ø–∏—Å–∫–∏: - –∏–ª–∏ 1. 2. 3. –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π
- –ö–æ–¥: ```—è–∑—ã–∫ –¥–ª—è –±–ª–æ–∫–æ–≤ –∫–æ–¥–∞, `–∫–æ–¥` –¥–ª—è inline
- –í—ã–¥–µ–ª—è–π **–≤–∞–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã** –∂–∏—Ä–Ω—ã–º

–ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–≤–µ—Ç–æ–≤:
- –û—Ç–≤–µ—á–∞–π –Ω–∞ —è–∑—ã–∫–µ –≤–æ–ø—Ä–æ—Å–∞
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º
- –ü—Ä–∏–≤–æ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã –≥–¥–µ —É–º–µ—Å—Ç–Ω–æ
- –£—á–∏—Ç—ã–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –¥–∏–∞–ª–æ–≥–µ"""


# ============================================================================
# API KEY RESOLUTION
# ============================================================================

def get_openai_api_key_from_assistant(
    db: Session,
    assistant_id: Optional[str]
) -> Optional[str]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç OpenAI API –∫–ª—é—á –∏–∑ –º–æ–¥–µ–ª–∏ User —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫—É:
    assistant_id ‚Üí GeminiAssistantConfig ‚Üí user_id ‚Üí User ‚Üí openai_api_key
    
    Args:
        db: Database session
        assistant_id: UUID of Gemini assistant
        
    Returns:
        OpenAI API key or None if not found
    """
    if not assistant_id or not db:
        logger.warning("[LLM-WS] No assistant_id or db provided, falling back to env")
        return os.environ.get('OPENAI_API_KEY')
    
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º Gemini –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        try:
            assistant_uuid = uuid.UUID(assistant_id)
            assistant = db.query(GeminiAssistantConfig).get(assistant_uuid)
        except ValueError:
            # –ï—Å–ª–∏ –Ω–µ UUID, –ø—Ä–æ–±—É–µ–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É
            assistant = db.query(GeminiAssistantConfig).filter(
                GeminiAssistantConfig.id.cast(str) == assistant_id
            ).first()
        
        if not assistant:
            logger.warning(f"[LLM-WS] Assistant not found: {assistant_id}")
            return os.environ.get('OPENAI_API_KEY')
        
        logger.info(f"[LLM-WS] Found assistant: {getattr(assistant, 'name', assistant_id)}")
        
        # 2. –ü–æ–ª—É—á–∞–µ–º user_id –∏–∑ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        if not assistant.user_id:
            logger.warning(f"[LLM-WS] Assistant has no user_id")
            return os.environ.get('OPENAI_API_KEY')
        
        # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user = db.query(User).get(assistant.user_id)
        
        if not user:
            logger.warning(f"[LLM-WS] User not found: {assistant.user_id}")
            return os.environ.get('OPENAI_API_KEY')
        
        logger.info(f"[LLM-WS] Found user: {user.email}")
        
        # 4. –ü–æ–ª—É—á–∞–µ–º OpenAI –∫–ª—é—á
        api_key = user.openai_api_key
        
        if api_key:
            logger.info(f"[LLM-WS] ‚úÖ OpenAI API key loaded from User model: {api_key[:10]}...{api_key[-4:]}")
            return api_key
        else:
            logger.warning(f"[LLM-WS] User {user.email} has no OpenAI API key configured")
            # Fallback to environment variable
            env_key = os.environ.get('OPENAI_API_KEY')
            if env_key:
                logger.info(f"[LLM-WS] ‚ö†Ô∏è Falling back to environment OPENAI_API_KEY")
            return env_key
            
    except Exception as e:
        logger.error(f"[LLM-WS] Error getting API key: {e}")
        return os.environ.get('OPENAI_API_KEY')


# ============================================================================
# HISTORY PROCESSING (v3.0)
# ============================================================================

def process_chat_history(history: List[Dict[str, Any]]) -> List[Dict[str, str]]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞.
    
    Args:
        history: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞
        
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è OpenAI API
    """
    if not history:
        return []
    
    processed = []
    
    for msg in history:
        if not isinstance(msg, dict):
            continue
            
        role = msg.get("role", "").strip().lower()
        content = msg.get("content", "").strip()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è role
        if role not in ("user", "assistant"):
            continue
            
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        if not content:
            continue
            
        processed.append({
            "role": role,
            "content": content
        })
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π
    if len(processed) > LLMStreamConfig.MAX_HISTORY_MESSAGES:
        processed = processed[-LLMStreamConfig.MAX_HISTORY_MESSAGES:]
        logger.info(f"[LLM-WS] History trimmed to {LLMStreamConfig.MAX_HISTORY_MESSAGES} messages")
    
    return processed


# ============================================================================
# HANDLER
# ============================================================================

async def handle_openai_streaming_websocket(
    websocket: WebSocket,
    assistant_id: Optional[str] = None,
    db: Optional[Session] = None
) -> None:
    """
    WebSocket handler –¥–ª—è LLM —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞.
    
    üîß v2.0: OpenAI API key –±–µ—Ä—ë—Ç—Å—è –∏–∑ –º–æ–¥–µ–ª–∏ User —á–µ—Ä–µ–∑ assistant_id.
    üîß v3.0: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞ (–¥–æ 5 –ø–∞—Ä —Å–æ–æ–±—â–µ–Ω–∏–π).
    
    Args:
        websocket: WebSocket connection
        assistant_id: UUID of Gemini assistant for API key lookup
        db: Database session
    """
    client_id = str(uuid.uuid4())[:8]
    
    logger.info(f"[LLM-WS] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    logger.info(f"[LLM-WS] üîå NEW CONNECTION (v3.0)")
    logger.info(f"[LLM-WS]    Client ID: {client_id}")
    logger.info(f"[LLM-WS]    Assistant ID: {assistant_id}")
    logger.info(f"[LLM-WS]    API Key Source: User model")
    logger.info(f"[LLM-WS]    History Support: ‚úÖ (max {LLMStreamConfig.MAX_HISTORY_MESSAGES} msgs)")
    logger.info(f"[LLM-WS] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    
    # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á –∏–∑ User –º–æ–¥–µ–ª–∏
    api_key = get_openai_api_key_from_assistant(db, assistant_id)
    
    try:
        await websocket.accept()
        logger.info(f"[LLM-WS] ‚úÖ Connected: {client_id}")
        
        if not api_key:
            logger.error(f"[LLM-WS] ‚ùå No OpenAI API key available")
            await websocket.send_json({
                "type": "error",
                "error": "OpenAI API key not configured. Please add your OpenAI API key in Settings.",
                "error_code": "no_api_key"
            })
            await websocket.close(code=1008, reason="No API key")
            return
        
        await websocket.send_json({
            "type": "connection_status",
            "status": "connected",
            "client_id": client_id,
            "api_key_source": "user_model" if assistant_id else "environment",
            "history_support": True,
            "max_history": LLMStreamConfig.MAX_HISTORY_MESSAGES
        })
        
        # Main loop
        while True:
            try:
                data = await websocket.receive_json()
                msg_type = data.get("type")
                
                if msg_type == "llm.query":
                    query = data.get("query", "")
                    request_id = data.get("request_id", f"req_{uuid.uuid4().hex[:8]}")
                    
                    # üÜï v3.0: –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
                    raw_history = data.get("history", [])
                    history = process_chat_history(raw_history)
                    
                    if query:
                        await stream_llm_response(
                            websocket=websocket,
                            query=query,
                            request_id=request_id,
                            api_key=api_key,
                            history=history  # üÜï –ü–µ—Ä–µ–¥–∞—ë–º –∏—Å—Ç–æ—Ä–∏—é
                        )
                
                elif msg_type == "ping":
                    await websocket.send_json({"type": "pong"})
                    
            except WebSocketDisconnect:
                logger.info(f"[LLM-WS] Disconnected: {client_id}")
                break
            except json.JSONDecodeError as e:
                logger.warning(f"[LLM-WS] Invalid JSON: {e}")
                continue
            except Exception as e:
                logger.error(f"[LLM-WS] Error in main loop: {e}")
                try:
                    await websocket.send_json({
                        "type": "error",
                        "error": str(e)[:200]
                    })
                except:
                    break
                    
    except Exception as e:
        logger.error(f"[LLM-WS] Connection error: {e}")
    finally:
        logger.info(f"[LLM-WS] üëã Closed: {client_id}")


async def stream_llm_response(
    websocket: WebSocket,
    query: str,
    request_id: str,
    api_key: str,
    history: List[Dict[str, str]] = None  # üÜï v3.0
) -> None:
    """
    –°—Ç—Ä–∏–º–∏—Ç –æ—Ç–≤–µ—Ç –æ—Ç OpenAI –Ω–∞ WebSocket.
    
    Args:
        websocket: WebSocket connection
        query: User query
        request_id: Request ID for tracking
        api_key: OpenAI API key (from User model)
        history: Chat history (list of {role, content} dicts)
    """
    if history is None:
        history = []
    
    start_time = time.time()
    full_content = ""
    buffer = ""
    last_flush = time.time()
    messages_sent = 0
    
    logger.info(f"[LLM-WS] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    logger.info(f"[LLM-WS] üöÄ STREAM START")
    logger.info(f"[LLM-WS]    Request ID: {request_id}")
    logger.info(f"[LLM-WS]    Query: {query[:100]}{'...' if len(query) > 100 else ''}")
    logger.info(f"[LLM-WS]    History: {len(history)} messages")  # üÜï
    logger.info(f"[LLM-WS]    Model: {LLMStreamConfig.MODEL}")
    logger.info(f"[LLM-WS] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    
    try:
        # Start event
        await websocket.send_json({
            "type": "llm.stream.start",
            "request_id": request_id,
            "query": query,
            "model": LLMStreamConfig.MODEL,
            "history_count": len(history)  # üÜï
        })
        
        # üÜï v3.0: –§–æ—Ä–º–∏—Ä—É–µ–º messages —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT}
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        for msg in history:
            messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
        messages.append({
            "role": "user",
            "content": query
        })
        
        logger.info(f"[LLM-WS]    Total messages to API: {len(messages)} (1 system + {len(history)} history + 1 current)")
        
        # Stream from OpenAI
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        payload = {
            "model": LLMStreamConfig.MODEL,
            "messages": messages,  # üÜï –¢–µ–ø–µ—Ä—å —Å –∏—Å—Ç–æ—Ä–∏–µ–π
            "max_tokens": LLMStreamConfig.MAX_TOKENS,
            "temperature": LLMStreamConfig.TEMPERATURE,
            "stream": True,
            "stream_options": {"include_usage": True}
        }
        
        timeout = aiohttp.ClientTimeout(
            total=LLMStreamConfig.REQUEST_TIMEOUT,
            connect=LLMStreamConfig.CONNECT_TIMEOUT
        )
        
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.post(
                LLMStreamConfig.OPENAI_API_URL,
                headers=headers,
                json=payload
            ) as response:
                
                if response.status != 200:
                    error_text = await response.text()
                    logger.error(f"[LLM-WS] ‚ùå OpenAI API error: {response.status}")
                    logger.error(f"[LLM-WS]    Response: {error_text[:500]}")
                    
                    # Parse OpenAI error
                    try:
                        error_data = json.loads(error_text)
                        error_message = error_data.get("error", {}).get("message", error_text)
                    except:
                        error_message = error_text[:200]
                    
                    raise Exception(f"OpenAI API error ({response.status}): {error_message}")
                
                tokens_used = 0
                
                logger.info(f"[LLM-WS] üì• Streaming response...")
                
                async for line in response.content:
                    line = line.decode('utf-8').strip()
                    
                    if not line or not line.startswith("data: "):
                        continue
                    
                    data_str = line[6:]
                    
                    if data_str == "[DONE]":
                        logger.info(f"[LLM-WS] üì• Stream finished")
                        break
                    
                    try:
                        data = json.loads(data_str)
                        
                        choices = data.get("choices", [])
                        if choices:
                            delta = choices[0].get("delta", {})
                            content = delta.get("content")
                            
                            if content:
                                full_content += content
                                buffer += content
                                
                                # Flush buffer conditions
                                current_time = time.time()
                                should_flush = (
                                    len(buffer) >= LLMStreamConfig.BUFFER_MIN_CHARS or
                                    (current_time - last_flush) >= LLMStreamConfig.BUFFER_MAX_WAIT or
                                    buffer.rstrip().endswith(('.', '!', '?', '\n', '„ÄÇ', 'ÔºÅ', 'Ôºü'))
                                )
                                
                                if should_flush and buffer:
                                    await websocket.send_json({
                                        "type": "llm.stream.delta",
                                        "request_id": request_id,
                                        "content": buffer
                                    })
                                    messages_sent += 1
                                    buffer = ""
                                    last_flush = current_time
                                    
                                    # Small delay to prevent browser overload
                                    await asyncio.sleep(0.01)
                        
                        usage = data.get("usage")
                        if usage:
                            tokens_used = usage.get("total_tokens", 0)
                            
                    except json.JSONDecodeError:
                        continue
        
        # Flush remaining buffer
        if buffer:
            await websocket.send_json({
                "type": "llm.stream.delta",
                "request_id": request_id,
                "content": buffer
            })
            messages_sent += 1
        
        # Done event
        duration_ms = int((time.time() - start_time) * 1000)
        
        await websocket.send_json({
            "type": "llm.stream.done",
            "request_id": request_id,
            "full_content": full_content,
            "tokens_used": tokens_used,
            "duration_ms": duration_ms,
            "messages_sent": messages_sent,
            "model": LLMStreamConfig.MODEL,
            "history_count": len(history)  # üÜï
        })
        
        logger.info(f"[LLM-WS] ‚úÖ STREAM COMPLETE")
        logger.info(f"[LLM-WS]    Duration: {duration_ms}ms")
        logger.info(f"[LLM-WS]    Content: {len(full_content)} chars")
        logger.info(f"[LLM-WS]    Tokens: {tokens_used}")
        logger.info(f"[LLM-WS]    Messages: {messages_sent}")
        logger.info(f"[LLM-WS]    History used: {len(history)} msgs")  # üÜï
        
    except asyncio.TimeoutError:
        error_msg = "Request timeout - OpenAI API did not respond in time"
        logger.error(f"[LLM-WS] ‚ùå TIMEOUT: {error_msg}")
        await websocket.send_json({
            "type": "llm.stream.error",
            "request_id": request_id,
            "error": error_msg,
            "error_code": "timeout"
        })
        
    except aiohttp.ClientError as e:
        error_msg = f"Connection error: {str(e)}"
        logger.error(f"[LLM-WS] ‚ùå CONNECTION ERROR: {error_msg}")
        await websocket.send_json({
            "type": "llm.stream.error",
            "request_id": request_id,
            "error": error_msg,
            "error_code": "connection_error"
        })
        
    except Exception as e:
        error_msg = str(e)[:200]
        logger.error(f"[LLM-WS] ‚ùå ERROR: {e}")
        await websocket.send_json({
            "type": "llm.stream.error",
            "request_id": request_id,
            "error": error_msg,
            "error_code": "internal_error"
        })
